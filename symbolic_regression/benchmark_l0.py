"""Trains the deep symbolic regression architecture on given functions to produce a simple equation that describes
the dataset. Uses L_0 regularization for the EQL network."""

import pickle
import numpy as np
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from utils import pretty_print, functions
from utils.symbolic_network import SymbolicNetL0
# from utils.symbolic_network_KAN import SymbolicNetL0
from inspect import signature
import time
import argparse
import tqdm
from tqdm import trange
from gplearn.genetic import SymbolicRegressor
from sklearn.metrics import r2_score

from KAN.KAN import *
from KAN.utils import evaluate_complexity

from gflownet.gflownet import *
from gflownet.utils import *
from gflownet.env   import *



N_TRAIN = 100 #10_000  #1_000_000 # Size of training dataset
N_VAL = 10  #1000 #100_000    # Size of validation dataset
DOMAIN = (-1, 1)    # Domain of dataset - range from which we sample x
# DOMAIN = np.array([[0, -1, -1], [1, 1, 1]])   # Use this format if each input variable has a different domain
N_TEST = 10        # Size of test dataset
DOMAIN_TEST = (-2, 2)   # Domain of test dataset - should be larger than training domain to test extrapolation
NOISE_SD = 0        # Standard deviation of noise for training dataset
var_names = ["x", "y", "z", "a", "b", "c", "d", "e", "f"]

# Standard deviation of random distribution for weight initializations.
init_sd_first = 0.1
init_sd_last = 1.0
init_sd_middle = 0.5
# init_sd_first = 0.5
# init_sd_last = 0.5
# init_sd_middle = 0.5
# init_sd_first = 0.1
# init_sd_last = 0.1
# init_sd_middle = 0.1


def generate_data(func, N, range_min=DOMAIN[0], range_max=DOMAIN[1]):
    """Generates datasets."""
    x_dim = len(signature(func).parameters)     # Number of inputs to the function, or, dimensionality of x
    x = (range_max - range_min) * torch.rand([N, x_dim]) + range_min
    y = torch.tensor([[func(*x_i)] for x_i in x])
    return x, y


class Benchmark:
    """Benchmark object just holds the results directory (results_dir) to save to and the hyper-parameters. So it is
    assumed all the results in results_dir share the same hyper-parameters. This is useful for benchmarking multiple
    functions with the same hyper-parameters."""
    def __init__(self, results_dir, n_layers=2, reg_weight=5e-3, learning_rate=1e-2,
                 n_epochs1=10001, n_epochs2=10001):
        """Set hyper-parameters"""
        self.activation_funcs = [
            *[functions.Constant()] * 2,
            *[functions.Identity()] * 4,
            *[functions.Square()] * 4,
            *[functions.Sin()] * 2,
            *[functions.Exp()] * 2,
            *[functions.Sigmoid()] * 2,
            # *[functions.Log()] * 2,
            # *[functions.Sqrt()] * 2,
            *[functions.Product(1.0)] * 2,
            
        ]

        self.n_layers = n_layers                # Number of hidden layers
        self.reg_weight = reg_weight            # Regularization weight
        self.learning_rate = learning_rate
        self.summary_step = 10              # Number of iterations at which to print to screen
        self.n_epochs1 = n_epochs1
        self.n_epochs2 = n_epochs2

        if not os.path.exists(results_dir):
            os.makedirs(results_dir)
        self.results_dir = results_dir

        # Save hyperparameters to file
        result = {
            "learning_rate": self.learning_rate,
            "summary_step": self.summary_step,
            "n_epochs1": self.n_epochs1,
            "n_epochs2": self.n_epochs2,
            "activation_funcs_name": [func.name for func in self.activation_funcs],
            "n_layers": self.n_layers,
            "reg_weight": self.reg_weight,
        }
        with open(os.path.join(self.results_dir, 'params.pickle'), "wb+") as f:
            pickle.dump(result, f)

    def benchmark(self, func, func_name, trials):
        """Benchmark the EQL network on data generated by the given function. Print the results ordered by test error.

        Arguments:
            func: lambda function to generate dataset
            func_name: string that describes the function - this will be the directory name
            trials: number of trials to train from scratch. Will save the results for each trial.
        """

        print("Starting benchmark for function:\t%s" % func_name)
        print("==============================================")

        # Create a new sub-directory just for the specific function
        func_dir = os.path.join(self.results_dir, func_name)
        if not os.path.exists(func_dir):
            os.makedirs(func_dir)

        # Train network!
        expr_list, error_test_list = self.train(func, func_name, trials, func_dir)

        # Sort the results by test error (increasing) and print them to file
        # This allows us to easily count how many times it fit correctly.
        error_expr_sorted = sorted(zip(error_test_list, expr_list))     # List of (error, expr)
        error_test_sorted = [x for x, _ in error_expr_sorted]   # Separating out the errors
        expr_list_sorted = [x for _, x in error_expr_sorted]    # Separating out the expr

        fi = open(os.path.join(self.results_dir, 'eq_summary.txt'), 'a')
        fi.write("\n{}\n".format(func_name))
        for i in range(trials):
            fi.write("[%f]\t\t%s\n" % (error_test_sorted[i], str(expr_list_sorted[i])))
        fi.close()

    def train(self, func, func_name='', trials=1, func_dir='results/test'):
        """Train the network to find a given function"""

        use_cuda = torch.cuda.is_available()
        device = torch.device("cuda" if use_cuda else "cpu")
        print("Use cuda:", use_cuda, "Device:", device)

        x, y = generate_data(func, N_TRAIN)
        data, target = x.to(device), y.to(device)
        # x_val, y_val = generate_data(func, N_VAL)
        x_test, y_test = generate_data(func, N_TEST, range_min=DOMAIN_TEST[0], range_max=DOMAIN_TEST[1])
        test_data, test_target = x_test.to(device), y_test.to(device)

        # Setting up the symbolic regression network
        x_dim = len(signature(func).parameters)  # Number of input arguments to the function

        width = len(self.activation_funcs)
        n_double = functions.count_double(self.activation_funcs)

        # Arrays to keep track of various quantities as a function of epoch
        loss_list = []          # Total loss (MSE + regularization)
        error_list = []         # MSE
        reg_list = []           # Regularization
        error_test_list = []    # Test error

        error_test_final = []
        eq_list = []
        
        pbar = trange(trials)
        # pbar.set_description("Training on function " + func_name)
        print("Training on function " + func_name)
        for trial in pbar:
            # print("Training on function " + func_name + " Trial " + str(trial+1) + " out of " + str(trials))

            # reinitialize for each trial
            net = SymbolicNetL0(self.n_layers,
                                funcs=self.activation_funcs,
                                initial_weights=[
                                    # kind of a hack for truncated normal
                                    torch.fmod(torch.normal(0, init_sd_first, size=(x_dim, width + n_double)), 2),
                                    torch.fmod(torch.normal(0, init_sd_middle, size=(width, width + n_double)), 2),
                                    torch.fmod(torch.normal(0, init_sd_middle, size=(width, width + n_double)), 2),
                                    torch.fmod(torch.normal(0, init_sd_last, size=(width, 1)), 2)
                                ]).to(device)
            
            kan = KAN(width=[x_dim,1,1], grid=2, k=4)

            def count_parameters(model):
                return sum(p.numel() for p in model.parameters() if p.requires_grad)

            print(f"DSR parameters: {count_parameters(net)}")
            print(f"KAN parameters: {count_parameters(kan)}")
            
            dataset = {'train_input': data, 'train_label': target, 'test_input': test_data, 'test_label': test_target}
            
            zero_th = 100
            kan_results = kan.train(dataset, opt="Adam", steps=self.n_epochs1 + self.n_epochs2 + zero_th, lr=self.learning_rate)
            print(f"\nKAN MSE {np.array(kan_results['test_loss'])[-1].item()}")
            print(f"KAN R^2: {kan_results['test_R^2'][-1]}")
            kan.plot()
            
            
            est_gp = SymbolicRegressor(population_size=5000,
                           generations=20, stopping_criteria=0.01,
                           p_crossover=0.7, p_subtree_mutation=0.1,
                           p_hoist_mutation=0.05, p_point_mutation=0.1,
                           max_samples=0.9, verbose=0,
                           parsimony_coefficient=0.01, random_state=0
                           )
            est_gp.fit(data, target)
            y_gp = est_gp.predict(test_data)
            
            def ensure_cpu(array):
                if hasattr(array, 'cpu'):
                    return array.cpu().numpy()
                return np.array(array)

            def ensure_1d(array):
                return array.ravel()

            # Assuming test_target and y_gp are tensors or numpy arrays
            test_target_cpu = ensure_cpu(test_target)
            y_gp_cpu = ensure_cpu(y_gp)

            # Ensure they are 1D arrays
            test_target_1d = ensure_1d(test_target_cpu)
            y_gp_1d = ensure_1d(y_gp_cpu)
                    
            
            print(f"GPLearn MSE: {np.mean((test_target_1d - y_gp_1d)**2)}")
            print(f"GPLearn R^2: {r2_score(test_target_1d, y_gp_1d)}")
            # kan = kan.prune(threshold=0.1)
            # kan(dataset['train_input'])
            # kan.plot()
            # lib = ['x^2','x', 'x^3','exp', 'log', 'sqrt',]
            # kan.auto_symbolic()
            # formula1 = kan.symbolic_formula()
    
            # Evaluate the complexity of the expression
            # complexity_score = evaluate_complexity(formula1[0][0])
            # print("KAN equation complexity:", complexity_score)
            # print("KAN equation:", formula1[0][0])
            
            loss_val = np.nan
            while np.isnan(loss_val):
                # training restarts if gradients blow up
                criterion = nn.MSELoss()
                # optimizer = optim.RMSprop(net.parameters(),
                #                           lr=self.learning_rate * 10,
                #                           alpha=0.9,  # smoothing constant
                #                           eps=1e-10,
                #                           momentum=0.0,
                #                           centered=False)
                
                optimizer = optim.Adam(net.parameters(), lr=self.learning_rate, weight_decay=0.005)

                # adapative learning rate
                lmbda = lambda epoch: 0.1
                # scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
                scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=3, verbose=True)
                # for param_group in optimizer.param_groups:
                #     print("Learning rate: %f" % param_group['lr'])

                t0 = time.time()

                # 0th warmup stage, then 2 stages of training with decreasing learning rate
               
                for epoch in range(self.n_epochs1 + self.n_epochs2 + zero_th):
                    optimizer.zero_grad()  # zero the parameter gradients
                    outputs = net(data)  # forward pass
                    
                    mse_loss = criterion(outputs, target)
                    # print(outputs.mean(), target.mean(), mse_loss)

                    reg_loss = net.get_loss()
                    loss = mse_loss + self.reg_weight * reg_loss
                    
                    loss.backward()
                    optimizer.step()
                   

                    if epoch % self.summary_step == 0:
                        error_val = mse_loss.item()
                        reg_val = reg_loss.item()
                        loss_val = loss.item()
                        error_list.append(error_val)
                        reg_list.append(reg_val)
                        loss_list.append(loss_val)

                        with torch.no_grad():  # test error
                            test_outputs = net(test_data)
                            test_loss = F.mse_loss(test_outputs, test_target)
                            test_r2 = r2_score(test_target.cpu().numpy(), test_outputs.cpu().numpy())
                            error_test_val = torch.sqrt(test_loss).item()
                            error_test_list.append(error_test_val)

                            # print("Epoch: %d\tTotal training loss: %f\tTest error: %f" % (epoch, loss_val, error_test_val))
                            # pbar.set_description(f"func: {func_name}, trial: {trial+1}, epoch: {epoch}, loss: {loss_val:.4f}, test error: {error_test_val:.4f}")
                            # print(f"func: {func_name}, trial: {trial+1}, epoch: {epoch}, loss: {loss_val:.2e}, lr: {optimizer.param_groups[0]['lr']:.2e}, test error: {error_test_val:.2e}")
                            
                        if np.isnan(loss_val):  # If loss goes to NaN, restart training
                            break

                    if epoch == 2000:
                        scheduler.step(loss)  # lr /= 10
                    elif epoch == self.n_epochs1 + 2000:
                        scheduler.step(loss)    # lr /= 10 again
                print(f"EQL MSE {sum(error_test_list[-5:])/5}")   
                print(f"EQL R^2 {test_r2}")
                scheduler.step(loss)  # lr /= 10 again

                t1 = time.time()

            tot_time = t1 - t0
            print(f"total training: ", tot_time, "\n")
            
            
            # Print the expressions
            with torch.no_grad():
                weights = net.get_weights()
                expr = pretty_print.network(weights, self.activation_funcs, var_names[:x_dim])
                print("EQL equation: ", expr)
                print("EQL equation complexity: ", evaluate_complexity(expr))
                # print("EQL equation: ", expr)

            # Save results
            trial_file = os.path.join(func_dir, 'trial%d.pickle' % trial)
            results = {
                "weights": weights,
                "loss_list": loss_list,
                "error_list": error_list,
                "reg_list": reg_list,
                "error_test": error_test_list,
                "expr": expr,
                "runtime": tot_time
            }
            with open(trial_file, "wb+") as f:
                pickle.dump(results, f)

            error_test_final.append(error_test_list[-1])
            eq_list.append(expr)
            
        def calculate_iqr_threshold(data, factor=1.5):
            """Calculate the upper threshold for outliers using IQR."""
            q1 = np.percentile(data, 25)
            q3 = np.percentile(data, 75)
            iqr = q3 - q1
            threshold = q3 + factor * iqr
            return threshold

        # Calculate thresholds for each dataset
        kan_train_threshold = calculate_iqr_threshold(kan_results['train_loss'])
        kan_test_threshold = calculate_iqr_threshold(kan_results['test_loss'])
        dsr_train_threshold = calculate_iqr_threshold(results['loss_list'])
        dsr_test_threshold = calculate_iqr_threshold(results['error_test'])

        # Clip the values in the datasets to the calculated thresholds
        kan_train_loss_clipped = np.clip(kan_results['train_loss'], a_min=None, a_max=kan_train_threshold)
        kan_test_loss_clipped = np.clip(kan_results['test_loss'], a_min=None, a_max=kan_test_threshold)
        dsr_train_loss_clipped = np.clip(results['loss_list'], a_min=None, a_max=dsr_train_threshold)
        dsr_test_loss_clipped = np.clip(results['error_test'], a_min=None, a_max=dsr_test_threshold)

        # Plot the losses for both KAN and DSR
        plt.figure(figsize=(10, 5))
        cutoff = 0

        # Plot KAN results
        plt.plot(kan_train_loss_clipped[cutoff:], label='KAN Train Loss', color='blue')
        plt.plot(kan_test_loss_clipped[cutoff:], label='KAN Test Loss', color='blue', linestyle='dashed')

        # Plot DSR results
        plt.plot(dsr_train_loss_clipped[cutoff:], label='DSR Train Loss', color='orange')
        plt.plot(dsr_test_loss_clipped[cutoff:], label='DSR Test Loss', color='orange', linestyle='dashed')

        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.title(f'Train and Test Loss for {func_name}')
        plt.legend()
        plt.grid(True)

        # Save the plot
        plt.savefig(f'loss_plot_KAN_DSR_{func_name}.png')

        return eq_list, error_test_final


if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="Train the EQL network.")
    parser.add_argument("--results-dir", type=str, default='results/benchmark/test')
    parser.add_argument("--n-layers", type=int, default=3, help="Number of hidden layers, L")
    parser.add_argument("--reg-weight", type=float, default=5e-3, help='Regularization weight, lambda')
    parser.add_argument('--learning-rate', type=float, default=1e-2, help='Base learning rate for training')
    parser.add_argument("--n-epochs1", type=int, default=500, help="Number of epochs to train the first stage")
    parser.add_argument("--n-epochs2", type=int, default=500,
                        help="Number of epochs to train the second stage, after freezing weights.")

    args = parser.parse_args()
    kwargs = vars(args)
    print(kwargs)

    if not os.path.exists(kwargs['results_dir']):
        os.makedirs(kwargs['results_dir'])
    meta = open(os.path.join(kwargs['results_dir'], 'args.txt'), 'a')
    import json
    meta.write(json.dumps(kwargs))
    meta.close()

    bench = Benchmark(**kwargs)
    trial = 1
    bench.benchmark(lambda x: x, func_name="x", trials=trial)
    # bench.benchmark(lambda x: x**2, func_name="x^2", trials=trial)
    # bench.benchmark(lambda x: x**3, func_name="x^3", trials=trial)
    # bench.benchmark(lambda x: np.sin(2*np.pi*x), func_name="sin(2pix)", trials=trial)
    # bench.benchmark(lambda x: np.exp(x), func_name="e^x", trials=trial)
    # bench.benchmark(lambda x, y: x*y, func_name="xy", trials=trial)
    # bench.benchmark(lambda x, y: np.sin(2 * np.pi * x) + np.sin(4*np.pi * y),
    #                 func_name="sin(2pix)+sin(4py)", trials=trial)
    # bench.benchmark(lambda x, y, z: 0.5*x*y + 0.5*z, func_name="0.5xy+0.5z", trials=trial)
    # bench.benchmark(lambda x, y, z: x**2 + y - 2*z, func_name="x^2+y-2z", trials=trial)
    # bench.benchmark(lambda x: np.exp(-x**2), func_name="e^-x^2", trials=trial)
    # bench.benchmark(lambda x: 1 / (1 + np.exp(-10*x)), func_name="sigmoid(10x)", trials=trial)
    # bench.benchmark(lambda x, y: x**2 + np.sin(2*np.pi*y), func_name="x^2+sin(2piy)", trials=trial)

    # # 3-layer functions
    # bench.benchmark(lambda x, y, z: (x+y*z)**3, func_name="(x+yz)^3", trials=trial)


    # NGUYEN equations
    # bench.benchmark(lambda x: x**3 + x**2 + x, func_name="x^3+x^2+x", trials=trial)
    # bench.benchmark(lambda x: x**5 + x**4 + x**3 + x**2 + x, func_name="x^5+x^4+x^3+x^2+x", trials=trial)
    # bench.benchmark(lambda x: np.sin(x**2) * np.cos(x) - 1, func_name="sin(x^2)*cos(x)-1", trials=trial)
    # bench.benchmark(lambda x: np.sin(x) + np.sin(x + x**2), func_name="sin(x)+sin(x+x^2)", trials=trial)
    # # bench.benchmark(lambda x: np.log(x + 1) + np.log(x**2 + 1), func_name="log(x+1)+log(x^2+1)", trials=trial)
    # # bench.benchmark(lambda x: np.sqrt(x), func_name="sqrt(x)", trials=trial) # Gives NaNs
    # bench.benchmark(lambda x0, x1: 2 * np.sin(x0) * np.cos(x1), func_name="2*sin(x0)*cos(x1)", trials=trial)
    # bench.benchmark(lambda x0, x1: x0**3 - 0.5 * x1**2, func_name="x0^3-0.5*x1^2", trials=trial)
    
    # # Feynman equations
    
    
    # bench.benchmark(lambda v1: np.exp(-v1**2/2)/np.sqrt(2*np.pi), func_name="I.6.2a", trials=trial)
    # bench.benchmark(lambda v1, v2: np.exp(-(v2/v1)**2/2)/(np.sqrt(2*np.pi)*v1), func_name="I.6.2", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: np.exp(-((v2-v3)/v1)**2/2)/(np.sqrt(2*np.pi)*v1), func_name="I.6.2b", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: np.sqrt((v2-v1)**2+(v4-v3)**2), func_name="I.8.14", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5, v6, v7, v8, v9: v3*v1*v2/((v5-v4)**2+(v7-v6)**2+(v9-v8)**2), func_name="I.9.18", trials=trial)
    # # bench.benchmark(lambda v1, v2, v3: v1/np.sqrt(1-v2**2/v3**2), func_name="I.10.7", trials=trial) # Gives NaNs
    # bench.benchmark(lambda v1, v2, v3, v4, v5, v6: v1*v4+v2*v5+v3*v6, func_name="I.11.19", trials=trial)
    # bench.benchmark(lambda v1, v2: v1*v2, func_name="I.12.1", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: v1*v2*v4/(4*np.pi*v3*v4**3), func_name="I.12.2", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: v1*v3/(4*np.pi*v2*v3**3), func_name="I.12.4", trials=trial)
    # bench.benchmark(lambda v1, v2: v1*v2, func_name="I.12.5", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5: v1*(v2+v3*v4*np.sin(v5)), func_name="I.12.11", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: 1/2*v1*(v2**2+v3**2+v4**2), func_name="I.13.4", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5: v5*v1*v2*(1/v4-1/v3), func_name="I.13.12", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: v1*v2*v3, func_name="I.14.3", trials=trial)
    # bench.benchmark(lambda v1, v2: 1/2*v1*v2**2, func_name="I.14.4", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: (v1-v2*v4)/np.sqrt(1-v2**2/v3**2), func_name="I.15.3x", trials=trial) # Gives NaNs
    # bench.benchmark(lambda v1, v2, v3, v4: (v4-v3*v1/v2**2)/np.sqrt(1-v3**2/v2**2), func_name="I.15.3t", trials=trial) # Gives NaNs
    # bench.benchmark(lambda v1, v2, v3: v1*v2/np.sqrt(1-v2**2/v3**2), func_name="I.15.1", trials=trial) # Gives NaNs
    # bench.benchmark(lambda v1, v2, v3: (v3+v2)/(1+v3*v2/v1**2), func_name="I.16.6", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: (v1*v3+v2*v4)/(v1+v2), func_name="I.18.4", trials=trial)
    # bench.benchmark(lambda v1, v2: v1*v2*np.sin(theta), func_name="I.18.12", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: v1*v2*v3*np.sin(theta), func_name="I.18.14", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: 1/2*v1*(v2**2+v3**2)*1/2*v4**2, func_name="I.24.6", trials=trial)
    # bench.benchmark(lambda v1, v2: v1/v2, func_name="I.25.13", trials=trial)
    # bench.benchmark(lambda v1, v2: np.arcsin(v1*np.sin(v2)), func_name="I.26.2", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: 1/(1/v1+v3/v2), func_name="I.27.6", trials=trial)
    # bench.benchmark(lambda v1, v2: v1/v2, func_name="I.29.4", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: np.sqrt(v1**2+v2**2-2*v1*v2*np.cos(v3-v4)), func_name="I.29.16", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: v1*np.sin(v3*v2/2)**2/np.sin(v2/2)**2, func_name="I.30.3", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: np.arcsin(v1/(v3*v2)), func_name="I.30.5", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: v1**2*v2**2/(6*np.pi*v3*v4**3), func_name="I.32.5", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5, v6: (1/2*v1*v2*v3**2)*(8*np.pi*v4**2/3)*(v5**4/(v5**2-v6**2)**2), func_name="I.32.17", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: v1*v2*v3/v4, func_name="I.34.8", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: v3/(1-v2/v1), func_name="I.34.1", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: (1+v2/v1)/np.sqrt(1-v2**2/v1**2)*v3, func_name="I.34.14", trials=trial) # Gives NaNs
    # bench.benchmark(lambda v1, v2: (v2/(2*np.pi))*v1, func_name="I.34.27", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: v1+v2+2*np.sqrt(v1*v2)*np.cos(v3), func_name="I.37.4", trials=trial) # Gives NaNs
    # bench.benchmark(lambda v1, v2, v3: 4*np.pi*epsilon*(v3/(2*np.pi))**2/(v1*v2**2), func_name="I.38.12", trials=trial)
    # bench.benchmark(lambda v1, v2: 3/2*v1*v2, func_name="I.39.1", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: 1/(v1-1)*v2*v3, func_name="I.39.11", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: v1*v4*v2/v3, func_name="I.39.22", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5, v6: v1*np.exp(-v2*v5*v3/(v6*v4)), func_name="I.40.1", trials=trial) # Gives NaNs
    # bench.benchmark(lambda v1, v2, v3, v4, v5: v3/(2*np.pi)*v1**3/(np.pi**2*v5**2*(np.exp((v3/(2*np.pi))*v1/(v4*v2))-1)), func_name="I.41.16", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: v1*v2*v3/v4, func_name="I.43.16", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: v1*v3*v2, func_name="I.43.31", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: 1/(v1-1)*v2*v4/v3, func_name="I.43.43", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5: v1*v2*v3*np.log(v5/v4), func_name="I.44.4", trials=trial) # Gives NaNs
    # bench.benchmark(lambda v1, v2, v3: np.sqrt(v1*v2/v3), func_name="I.47.23", trials=trial) # Gives NaNs
    # bench.benchmark(lambda v1, v2, v3: v1*v3**2/np.sqrt(1-v2**2/v3**2), func_name="I.48.2", trials=trial) # Gives NaNs
    # bench.benchmark(lambda v1, v2, v3, v4: v1*(np.cos(v2*v3)+v4*np.cos(v2*v3)**2), func_name="I.50.26", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5: v1*(v3-v2)*v4/v5, func_name="II.2.42", trials=trial)
    # bench.benchmark(lambda v1, v2: v1/(4*np.pi*v2**2), func_name="II.3.24", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: v1/(4*np.pi*v2*v3), func_name="II.4.23", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: 1/(4*np.pi*v1)*v2*np.cos(v3)/v4**2, func_name="II.6.11", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5, v6: v2/(4*np.pi*v1)*3*v6/v3**5*np.sqrt(v4**2+v5**2), func_name="II.6.15a", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: v2/(4*np.pi*v1)*3*np.cos(v3)*np.sin(v3)/v4**3, func_name="II.6.15b", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: 3/5*v1**2/(4*np.pi*v2*v3), func_name="II.8.7", trials=trial)
    # bench.benchmark(lambda v1, v2: v1*v2**2/2, func_name="II.8.31", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: v1/v2*1/(1+v3), func_name="II.10.9", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5: v1*v2/(v3*(v4**2-v5**2)), func_name="II.11.3", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5, v6: v1*(1+v5*v6*np.cos(v4)/(v2*v3)), func_name="II.11.17", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5: v1*v2**2*v3/(3*v4*v5), func_name="II.11.20", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: v1*v2/(1-(v1*v2/3))*v3*v4, func_name="II.11.27", trials=trial)
    # bench.benchmark(lambda v1, v2: 1+v1*v2/(1-(v1*v2/3)), func_name="II.11.28", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: 1/(4*np.pi*v1*v2**2)*2*v3/v4, func_name="II.13.17", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: v1/np.sqrt(1-v2**2/v3**2), func_name="II.13.23", trials=trial) # Gives NaNs
    # bench.benchmark(lambda v1, v2, v3: v1*v2/np.sqrt(1-v2**2/v3**2), func_name="II.13.34", trials=trial) # Gives NaNs
    # bench.benchmark(lambda v1, v2, v3: -v1*v2*np.cos(v3), func_name="II.15.4", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: -v1*v2*np.cos(v3), func_name="II.15.5", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5: v1/(4*np.pi*v2*v3*(1-v4/v5)), func_name="II.21.32", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: np.sqrt(v1**2/v2**2-np.pi**2/v3**2), func_name="II.24.17", trials=trial) # Gives NaNs
    # bench.benchmark(lambda v1, v2, v3: v1*v2*v3**2, func_name="II.27.16", trials=trial)
    # bench.benchmark(lambda v1, v2: v1*v2**2, func_name="II.27.18", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: v1*v2/(2*np.pi*v3), func_name="II.34.2a", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: v1*v2*v3/2, func_name="II.34.2", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: v1*v2*v3/(2*v4), func_name="II.34.11", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: v1*v2/(4*np.pi*v3), func_name="II.34.29a", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5: v1*v4*v5*v3/(v2/(2*np.pi)), func_name="II.34.29b", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5: v1/(np.exp(v4*v5/(v2*v3))+np.exp(-v4*v5/(v2*v3))), func_name="II.35.18", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5: v1*v2*np.tanh(v2*v3/(v4*v5)), func_name="II.35.21", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5, v6, v7, v8: v1*v2/(v3*v4)+(v1*v5)/(v6*v7**2*v3*v4)*v8, func_name="II.36.38", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: v1*(1+v3)*v2, func_name="II.37.1", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: v1*v2*v4/v3, func_name="II.38.3", trials=trial)
    # bench.benchmark(lambda v1, v2: v1/(2*(1+v2)), func_name="II.38.14", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: 1/(np.exp((v1/(2*np.pi))*v2/(v3*v4))-1), func_name="III.4.32", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: (v1/(2*np.pi))*v2/(np.exp((v1/(2*np.pi))*v2/(v3*v4))-1), func_name="III.4.33", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: 2*v1*v2/(v3/(2*np.pi)), func_name="III.7.38", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: np.sin(v1*v2/(v3/(2*np.pi)))**2, func_name="III.8.54", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5, v6: (v1*v2*v3/(v4/(2*np.pi)))*np.sin((v5-v6)*v3/2)**2/((v5-v6)*v3/2)**2, func_name="III.9.52", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: v1*np.sqrt(v2**2+v3**2+Bz**2), func_name="III.10.19", trials=trial)
    # bench.benchmark(lambda v1, v2: v1*(v2/(2*np.pi)), func_name="III.12.43", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: 2*v1*v2**2*v3/(v4/(2*np.pi)), func_name="III.13.18", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4, v5: v1*(np.exp(v2*v3/(v4*v5))-1), func_name="III.14.14", trials=trial) # Gives NaNs
    # bench.benchmark(lambda v1, v2, v3: 2*v1*(1-np.cos(v2*v3)), func_name="III.15.12", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: (v1/(2*np.pi))**2/(2*v2*v3**2), func_name="III.15.14", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: 2*np.pi*v1/(v2*v3), func_name="III.15.27", trials=trial)
    # bench.benchmark(lambda v1, v2, v3: v1*(1+v2*np.cos(v3)), func_name="III.17.37", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: -v1*v2**4/(2*(4*np.pi*epsilon)**2*(v3/(2*np.pi))**2)*(1/v4**2), func_name="III.19.51", trials=trial)
    # bench.benchmark(lambda v1, v2, v3, v4: -v1*v2*v3/v4, func_name="III.21.20", trials=trial)


    # import re
    # import pandas as pd
    
    # feynman_df = pd.read_csv('FeynmanEquations.csv')
    # # Function to extract variables and create lambda functions
    # def create_benchmark_lambda(formula, variables):
    #     # Extract variable names and replace in formula
    #     var_list = []
    #     for i, var in enumerate(variables):
    #         if pd.notna(var):
    #             var_name = f'v{i+1}'
    #             formula = re.sub(rf'\b{var}\b', var_name, formula)
    #             var_list.append(var_name)
        
    #     # Create lambda function string
    #     lambda_str = f"lambda {', '.join(var_list)}: {formula}"
    #     return lambda_str

    # # Extract and create benchmark functions
    # feynman_benchmarks = []
    # for index, row in feynman_df.iterrows():
    #     if pd.isna(row['# variables']):
    #         continue  # Skip rows where '# variables' is NaN

    #     formula = row['Formula']
    #     num_vars = int(row['# variables']) if not pd.isna(row['# variables']) else 0
    #     variables = [row[f'v{i+1}_name'] for i in range(num_vars)]
    #     func_name = row['Filename']
    #     lambda_str = create_benchmark_lambda(formula, variables)
    #     feynman_benchmarks.append((func_name, lambda_str))

    # # Print the benchmark functions
    # for func_name, lambda_str in feynman_benchmarks:
    #     print(f'bench.benchmark({lambda_str}, func_name="{func_name}", trials=trial)')